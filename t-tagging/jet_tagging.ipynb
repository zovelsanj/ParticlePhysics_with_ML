{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn import model_selection, metrics\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"dl4phys/top_tagging\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Fields\n",
    "The fields in the dataset have the following meaning:\n",
    "\n",
    "`E_i`: the energy of jet constituent i,\n",
    "\n",
    "`PX_i`: the x component of the jet constituent's momentum,\n",
    "\n",
    "`PY_i`: the y component of the jet constituent's momentum,\n",
    "\n",
    "`PZ_i`: the z component of the jet constituent's momentum,\n",
    "\n",
    "`truthE`: the energy of the top-quark,\n",
    "\n",
    "`truthPX`: the x component of the top quark's momentum, \n",
    "\n",
    "`truthPY`: the y component of the top quark's momentum,\n",
    "\n",
    "`truthPZ`: the z component of the top quark's momentum, \n",
    "\n",
    "`ttv`: a flag that indicates which split (train, validation, or test) that a jet belongs to. Redundant since each split is provided as a separate dataset, \n",
    "\n",
    "`is_signal_new`: the label for each jet. \n",
    "It indicates whether the jet is a top quark signal (1) or QCD background (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(['truthE', 'truthPX', 'truthPY', 'truthPZ', 'ttv']) #we won’t need the top-quark 4-vector columns and ttv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have each row consists of 4-vectors $(E_i, PX_i, PY_i,PZ_i)$ that correspond to the maximum 200 particles that make up each jet. Also, each jet has been padded with zeros, since most won’t have 200 particles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_format(\"pandas\")\n",
    "train_df, test_df = dataset[\"train\"][:], dataset[\"test\"][:]\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['is_signal_new'].value_counts() # check the number of 0s and 1s in 'is_signal_new' column\n",
    "train_df.isnull().sum() # check presence of missing data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "momentum_df = train_df.drop(columns=['is_signal_new'])\n",
    "momentum_df = momentum_df.filter(regex=r'E_\\d+$', axis=1)\n",
    "momentum_df = momentum_df.iloc[:500, :]\n",
    "label = kmeans.fit_predict(momentum_df)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = kmeans.cluster_centers_\n",
    "u_labels = np.unique(label)\n",
    "\n",
    "for i in u_labels:\n",
    "    plt.scatter(momentum_df.iloc[label == i , 0] , momentum_df.iloc[label == i , 1] , label = i)\n",
    "plt.scatter(centroids[:,0] , centroids[:,1] , s = 80, color = 'k')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        labels = list(self.dataset['is_signal_new'])\n",
    "        label = labels[idx] \n",
    "        sample_data = list(self.dataset.iloc[idx, :-1])\n",
    "        data = torch.tensor(sample_data, dtype=torch.float32)\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TabularDataset(train_df)\n",
    "test_dataset = TabularDataset(test_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBlock(nn.Sequential):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__(\n",
    "            nn.Linear(in_features=in_features, out_features=out_features),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(num_features=out_features)\n",
    "        )\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.t_tagger = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features=in_channels),\n",
    "            LinearBlock(in_features=in_channels, out_features=200),\n",
    "            LinearBlock(in_features=200, out_features=50),\n",
    "            nn.Linear(in_features=50, out_features=out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.t_tagger(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "model = Model(in_channels=800, out_channels=1)\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "val_size = int(0.1 * len(train_dataset))\n",
    "train_size = len(train_dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader =  DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader =  DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data, val_label = next(iter(val_dataloader))\n",
    "model(val_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dataloader, val_dataloader, model, lr=0.01, momentum=0.9, nesterov=False, n_epochs=30):\n",
    "    train_acc = []\n",
    "    train_loss = []\n",
    "    v_acc = []\n",
    "    v_loss = []\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, nesterov=nesterov)\n",
    "\n",
    "    for epoch in range(1, n_epochs):\n",
    "        print(\"-------------\\nEpoch {}:\\n\".format(epoch))\n",
    "\n",
    "        # Run **training***\n",
    "        loss, acc = run_epoch(train_dataloader, model.train(), optimizer)\n",
    "        print('Train loss: {:.6f} | Train accuracy: {:.6f}'.format(loss, acc))\n",
    "        train_loss.append(loss)\n",
    "        train_acc.append(acc)\n",
    "\n",
    "        # Run **validation**\n",
    "        val_loss, val_acc = run_epoch(val_dataloader, model.eval(), optimizer)\n",
    "        print('Val loss:   {:.6f} | Val accuracy:   {:.6f}'.format(val_loss, val_acc))\n",
    "        v_loss.append(val_loss)\n",
    "        v_acc.append(val_acc)\n",
    "\n",
    "        # Save model\n",
    "        # torch.save(model, 't-tagging.pt')\n",
    "    return val_acc, train_acc, train_loss, v_loss, v_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(dataset, model, optimizer):\n",
    "    losses = []\n",
    "    batch_accuracies = []\n",
    "\n",
    "    # If model is in train mode, use optimizer.\n",
    "    is_training = model.training\n",
    "    def compute_accuracy(predictions, y):\n",
    "        return np.mean(np.equal(predictions.detach().numpy(), y.numpy()))\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Iterate through batches\n",
    "    for data, label in dataset:\n",
    "        # Grab x and y\n",
    "        x, y = data[:32], label\n",
    "\n",
    "        # Get output predictions\n",
    "        out = model(x)\n",
    "        \n",
    "        # Predict and store accuracy\n",
    "        predictions = torch.argmax(out, dim=1)\n",
    "        batch_accuracies.append(compute_accuracy(predictions, y))\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(out.squeeze(), y.float())\n",
    "        losses.append(loss.data.item())\n",
    "        # print(f'loss: {loss}')\n",
    "\n",
    "        # If training, do an update.\n",
    "        if is_training:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Calculate epoch level scores\n",
    "    avg_loss = np.mean(losses)\n",
    "    avg_accuracy = np.mean(batch_accuracies)\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "n_epochs = 30\n",
    "train_model(train_dataloader, val_dataloader,  model, lr=lr, momentum=momentum, n_epochs=n_epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = run_epoch(test_batches, model.eval(), None)\n",
    "print (\"Loss on test set:\"  + str(loss) + \" Accuracy on test set: \" + str(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
